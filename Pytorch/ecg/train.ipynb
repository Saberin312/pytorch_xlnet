{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import keras\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import network\n",
    "import load\n",
    "import util\n",
    "\n",
    "MAX_EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_save_dir(dirname, experiment_name):\n",
    "    start_time = str(int(time.time())) + '-' + str(random.randrange(1000))\n",
    "    save_dir = os.path.join(dirname, experiment_name, start_time)\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    return save_dir\n",
    "\n",
    "def get_filename_for_saving(save_dir):\n",
    "    return os.path.join(save_dir,\n",
    "            \"{val_loss:.3f}-{val_acc:.3f}-{epoch:03d}-{loss:.3f}-{acc:.3f}.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, params):\n",
    "\n",
    "    print(\"Loading training set...\")\n",
    "    train = load.load_dataset(params['train'])\n",
    "    print(\"Loading dev set...\")\n",
    "    dev = load.load_dataset(params['dev'])\n",
    "    print(\"Building preprocessor...\")\n",
    "    preproc = load.Preproc(*train)\n",
    "    print(\"Training size: \" + str(len(train[0])) + \" examples.\")\n",
    "    print(\"Dev size: \" + str(len(dev[0])) + \" examples.\")\n",
    "\n",
    "\n",
    "    save_dir = make_save_dir(params['save_dir'], args.experiment)\n",
    "\n",
    "    util.save(preproc, save_dir)\n",
    "\n",
    "    params.update({\n",
    "        \"input_shape\": [None, 1],\n",
    "        \"num_categories\": len(preproc.classes)\n",
    "    })\n",
    "\n",
    "    model = network.build_network(**params)\n",
    "\n",
    "    stopping = keras.callbacks.EarlyStopping(patience=8)\n",
    "\n",
    "    reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "        factor=0.1,\n",
    "        patience=2,\n",
    "        min_lr=params[\"learning_rate\"] * 0.001)\n",
    "\n",
    "    checkpointer = keras.callbacks.ModelCheckpoint(\n",
    "        filepath=get_filename_for_saving(save_dir),\n",
    "        save_best_only=False)\n",
    "\n",
    "    batch_size = params.get(\"batch_size\", 32)\n",
    "\n",
    "    if params.get(\"generator\", False):\n",
    "        train_gen = load.data_generator(batch_size, preproc, *train)\n",
    "        dev_gen = load.data_generator(batch_size, preproc, *dev)\n",
    "        model.fit_generator(\n",
    "            train_gen,\n",
    "            steps_per_epoch=int(len(train[0]) / batch_size),\n",
    "            epochs=MAX_EPOCHS,\n",
    "            validation_data=dev_gen,\n",
    "            validation_steps=int(len(dev[0]) / batch_size),\n",
    "            callbacks=[checkpointer, reduce_lr, stopping])\n",
    "    else:\n",
    "        train_x, train_y = preproc.process(*train)\n",
    "        dev_x, dev_y = preproc.process(*dev)\n",
    "        model.fit(\n",
    "            train_x, train_y,\n",
    "            batch_size=batch_size,\n",
    "            epochs=MAX_EPOCHS,\n",
    "            validation_data=(dev_x, dev_y),\n",
    "            callbacks=[checkpointer, reduce_lr, stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"config_file\", help=\"path to config file\")\n",
    "    parser.add_argument(\"--experiment\", \"-e\", help=\"tag with experiment name\",\n",
    "                        default=\"default\")\n",
    "    args = parser.parse_args()\n",
    "    params = json.load(open(args.config_file, 'r'))\n",
    "    train(args, params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
