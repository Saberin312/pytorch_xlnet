{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Self Attention用于NLP,self即计算source 或者 target 的内部单词之间的相关性\n",
    "##pytorch实现Self Attention\n",
    "\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.functional as F\n",
    "\n",
    "import torchvision as tv\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import ToPILImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSelfAttn(\n",
      "  (linear): Linear(in_features=5, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# For summarizing a set of vectors into a single vector\n",
    "class LinearSelfAttn(nn.Module):\n",
    "    \"\"\"Self attention over a sequence:\n",
    "    * o_i = softmax(Wx_i) for x_i in X.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size):\n",
    "        super(LinearSelfAttn, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, 1)\n",
    "\n",
    "    def forward(self, x, x_mask):\n",
    "        \"\"\"\n",
    "        x = [batch, len, hdim]\n",
    "        x_mask = [batch, len]\n",
    "        \"\"\"\n",
    "        x = dropout(x, p=my_dropout_p, training=self.training)\n",
    "\n",
    "        x_flat = x.contiguous().view(-1, x.size(-1))\n",
    "        scores = self.linear(x_flat).view(x.size(0), x.size(1))\n",
    "        scores.data.masked_fill_(x_mask.data, -float('inf'))\n",
    "        alpha = F.softmax(scores, dim=1)\n",
    "        return alpha # [batch, len]\n",
    "    \n",
    "selfAttn=LinearSelfAttn(5)\n",
    "print(selfAttn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bmm: batch matrix multiplication\n",
    "# unsqueeze: add singleton dimension\n",
    "# squeeze: remove singleton dimension\n",
    "def weighted_avg(x, weights): \n",
    "    \"\"\" x = [batch, len, d]\n",
    "        weights = [batch, len]\n",
    "    \"\"\"\n",
    "    return weights.unsqueeze(1).bmm(x).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [batch,sentence_len,hidden_dim] -> [batch,sentence_len]\n",
    "sentence_weights = linear_self_attn(sentence_hiddens, sentence_mask) \n",
    "\n",
    "# [batch,hidden_dim]\n",
    "sentence_avg_hidden = weighted_avg(sentence_hiddens, sentence_weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
